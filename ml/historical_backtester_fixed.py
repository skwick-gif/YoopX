"""
××¢×¨×›×ª ×‘×“×™×§×” ×”×™×¡×˜×•×¨×™×ª - Phase 2 (××ª×•×§×Ÿ)
=====================================

×”×‘×§×¨ ×”×¨××©×™ ×œ×‘×™×¦×•×¢ ×‘×“×™×§×•×ª ×”×™×¡×˜×•×¨×™×•×ª ×¢× ××•×“×œ×™× ××•×ª×××™× ×œ×–××Ÿ.
××©×ª××© ×‘××¢×¨×›×ª ×”× ×ª×•× ×™× ×”×§×™×™××ª ×‘×“×™×•×§ ×›××• ×©×”×™× ×¢×•×‘×“×ª.

×ª×›×•× ×•×ª:
- ×‘×“×™×§×” ××•×˜×•××˜×™×ª ×©×œ ×ª×§×•×¤×•×ª ×©×•× ×•×ª
- ××™××•×Ÿ ××•×“×œ×™× ×¢× ×”×•×¨×™×–×•× ×™× ×©×•× ×™× (1/5/10 ×™××™×)
- ×¡×¨×™×§×” ×”×™×¡×˜×•×¨×™×ª ×•×—×™×©×•×‘ ×‘×™×¦×•×¢×™×
- ×”×©×•×•××ª ×ª×•×¦××•×ª ×œ×¤× ×™ ×•××—×¨×™

×”×¢×¨×” ×—×©×•×‘×” - ×–×¨×™××ª ×”× ×ª×•× ×™×:
===============================
1. ×›×¤×ª×•×¨ "Daily Update" â†’ ××•×¨×™×“ × ×ª×•× ×™× ×’×•×œ××™×™× (JSON) ×œ-raw_data
2. Processing Pipeline â†’ ×××™×¨ ×œ-Parquet ××•×‘× ×” ×‘-processed_data/_parquet 
3. ML Training â†’ ××©×ª××© ×‘× ×ª×•× ×™ ×”×¤××¨×§×˜ ×”××¢×•×‘×“×™× (×¤×•×¨××˜: date index + OHLCV columns)
4. Historical Backtesting â†’ ××©×ª××© ×‘××•×ª× × ×ª×•× ×™× ××¢×•×‘×“×™×

× ×ª×™×‘ × ×ª×•× ×™× × ×›×•×Ÿ: get_data_paths()['processed'] + '/_parquet'
×¤×•×¨××˜ × ×ª×•× ×™× × ×“×¨×©: DataFrame ×¢× date index ×•×§×•×œ×•× ×•×ª ['Open','High','Low','Close','Volume']
"""

import os
import sys
import json
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple
import pandas as pd

# ×”×•×¡×¤×ª × ×ª×™×‘ ×”×¤×¨×•×™×§×˜
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from ml.train_model import train_multi_horizon_model, filter_data_until_date


class HistoricalBacktester:
    """×‘×§×¨ ×œ×‘×™×¦×•×¢ ×‘×“×™×§×•×ª ×”×™×¡×˜×•×¨×™×•×ª ×¢× ××•×“×œ×™× ××•×ª×××™× - ××©×ª××© ×‘××¢×¨×›×ª ×”×§×™×™××ª"""
    
    def __init__(self, base_path: Optional[str] = None):
        self.base_path = base_path or os.getcwd()
        self.models_dir = os.path.join(self.base_path, 'ml', 'models', 'historical')
        self.results_dir = os.path.join(self.base_path, 'ml', 'backtest_results')
        
        # ×•×™×“×•× ×©×”×ª×™×§×™×•×ª ×§×™×™××•×ª
        os.makedirs(self.models_dir, exist_ok=True)
        os.makedirs(self.results_dir, exist_ok=True)
        
        # ×”×’×“×¨×ª ×œ×•×’
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)
        
    def run_historical_backtest(self, 
                              start_date: str, 
                              end_date: str, 
                              horizons: List[int] = [1, 5, 10],
                              algorithms: List[str] = ['rf']) -> Dict:
        """
        ××¨×™×¥ ×‘×“×™×§×” ×”×™×¡×˜×•×¨×™×ª ××œ××”
        
        Args:
            start_date: ×ª××¨×™×š ×”×ª×—×œ×” (YYYY-MM-DD)
            end_date: ×ª××¨×™×š ×¡×™×•× (YYYY-MM-DD) 
            horizons: ×¨×©×™××ª ×”×•×¨×™×–×•× ×™× ×œ×‘×“×™×§×”
            algorithms: ×¨×©×™××ª ××œ×’×•×¨×™×ª××™× ×œ×‘×“×™×§×”
            
        Returns:
            Dict: ×ª×•×¦××•×ª ×”×‘×“×™×§×” ×”×”×™×¡×˜×•×¨×™×ª
        """
        
        self.logger.info(f"ğŸš€ ××ª×—×™×œ ×‘×“×™×§×” ×”×™×¡×˜×•×¨×™×ª: {start_date} ×¢×“ {end_date}")
        
        results = {
            'config': {
                'start_date': start_date,
                'end_date': end_date,
                'horizons': horizons,
                'algorithms': algorithms
            },
            'daily_results': {},
            'summary': {}
        }
        
        # ×˜×¢×™× ×ª × ×ª×•× ×™× ×‘×××¦×¢×•×ª ×”××¢×¨×›×ª ×”×§×™×™××ª
        self.logger.info("ğŸ“¥ ×˜×•×¢×Ÿ × ×ª×•× ×™×...")
        all_data = self._load_all_data()
        
        if not all_data:
            self.logger.error("âŒ ×œ× × ××¦××• × ×ª×•× ×™×. ×”×¨×¥ Daily Update ×ª×—×™×œ×”.")
            return results
        
        # ×™×¦×™×¨×ª ×¨×©×™××ª ×ª××¨×™×›×™× ×œ×‘×“×™×§×”
        test_dates = self._generate_test_dates(start_date, end_date)
        
        total_tests = len(test_dates) * len(horizons) * len(algorithms)
        test_count = 0
        
        for test_date in test_dates:
            self.logger.info(f"ğŸ“… ×‘×•×“×§ ×ª××¨×™×š: {test_date}")
            
            date_results = {}
            
            for algorithm in algorithms:
                for horizon in horizons:
                    test_count += 1
                    progress = (test_count / total_tests) * 100
                    self.logger.info(f"ğŸ”„ [{progress:.1f}%] ××™××•×Ÿ {algorithm} ×œhorizon {horizon} ×™××™×")
                    
                    # ××™××•×Ÿ ××•×“×œ ×œ×ª××¨×™×š ×¡×¤×¦×™×¤×™
                    model_path = self._train_model_for_date(
                        test_date, horizon, algorithm, all_data
                    )
                    
                    if model_path:
                        # ×‘×™×¦×•×¢ ×¡×¨×™×§×” ×¢× ×”××•×“×œ ×”×—×“×©
                        scan_results = self._run_historical_scan(
                            model_path, test_date, horizon
                        )
                        
                        # ×©××™×¨×ª ×ª×•×¦××•×ª
                        key = f"{algorithm}_h{horizon}"
                        date_results[key] = {
                            'model_path': model_path,
                            'scan_results': scan_results
                        }
                    
            results['daily_results'][test_date] = date_results
            
            # ×©××™×¨×” ×‘×™× ×™×™×
            self._save_interim_results(results, test_date)
        
        # ×¡×™×›×•× ×”×ª×•×¦××•×ª
        self._generate_summary(results)
        
        # ×©××™×¨×” ×¡×•×¤×™×ª
        final_path = self._save_final_results(results)
        
        self.logger.info(f"âœ… ×‘×“×™×§×” ×”×•×©×œ××”! ×ª×•×¦××•×ª × ×©××¨×• ×‘: {final_path}")
        
        return results
    
    def _generate_test_dates(self, start_date: str, end_date: str, 
                           interval_days: int = 7) -> List[str]:
        """×™×•×¦×¨ ×¨×©×™××ª ×ª××¨×™×›×™× ×œ×‘×“×™×§×”"""
        start_dt = datetime.strptime(start_date, '%Y-%m-%d')
        end_dt = datetime.strptime(end_date, '%Y-%m-%d')
        
        dates = []
        current_dt = start_dt
        
        while current_dt <= end_dt:
            dates.append(current_dt.strftime('%Y-%m-%d'))
            current_dt += timedelta(days=interval_days)
        
        return dates
    
    def _load_all_data(self) -> Dict:
        """×˜×•×¢×Ÿ ××ª ×›×œ ×”× ×ª×•× ×™× ××”××¢×¨×›×ª ×”×§×™×™××ª - ×‘×“×™×•×§ ×›××• ×©×”××¢×¨×›×ª ×¢×•×‘×“×ª
        
        ×”××¢×¨×›×ª ×”×§×™×™××ª ×¢×•×‘×“×ª ×›×š:
        1. ×˜×•×¢× ×ª ×§×‘×¦×™ PARQUET (×©×¢×œ×•×œ×™× ×œ×”×›×™×œ JSON ×’×•×œ××™)
        2. ××¢×‘×™×¨×” ×“×¨×š ×¤×•× ×§×¦×™×•×ª ×”××¨×” (load_json logic) 
        3. ××¤×¢×™×œ×” maybe_adjust_with_adj
        4. ××—×–×™×¨×” DataFrame × ×§×™ ×¢× OHLCV ×•××™× ×“×§×¡ ×ª××¨×™×š
        """
        try:
            # ×©×™××•×© ×‘× ×ª×™×‘×™ ×”××¢×¨×›×ª ×”×§×™×™××ª
            from data.data_paths import get_data_paths
            from data.enhanced_verification import _load_processed_data_map
            
            paths = get_data_paths()
            processed_dir = paths['processed']
            
            self.logger.info(f"ğŸ“Š ×˜×•×¢×Ÿ × ×ª×•× ×™× ××ª×™×§×™×™×ª ×”××¢×•×‘×“×™×: {processed_dir}")
            
            # ×˜×¢×™× ×” ×‘×“×™×•×§ ×›××• ×©×”××¢×¨×›×ª ×”×§×™×™××ª ×¢×•×©×”
            raw_data_map = _load_processed_data_map(processed_dir)
            
            if not raw_data_map:
                self.logger.warning("âš ï¸ ×œ× × ××¦××• × ×ª×•× ×™× ××¢×•×‘×“×™×. ×”×¨×¥ Daily Update ×ª×—×™×œ×”.")
                return {}
            
            # ×¢×›×©×™×• × ×¢×‘×“ ××ª ×”× ×ª×•× ×™× ×”×’×•×œ××™×™× ×›××• ×©×”××¢×¨×›×ª ×”×§×™×™××ª ×¢×•×©×”
            processed_data_map = {}
            
            for ticker, raw_df in raw_data_map.items():
                try:
                    # ×‘×“×™×§×” ×× ×”× ×ª×•× ×™× ×›×‘×¨ ××¢×•×‘×“×™× ××• ×¦×¨×™×›×™× ×¢×™×‘×•×“
                    if self._is_clean_ohlcv_data(raw_df):
                        # × ×ª×•× ×™× ×›×‘×¨ × ×§×™×™× - ×¨×§ maybe_adjust_with_adj
                        from data.data_utils import maybe_adjust_with_adj
                        processed_df = maybe_adjust_with_adj(raw_df.copy(), use_adj=True)
                        processed_data_map[ticker] = processed_df
                        self.logger.debug(f"âœ“ {ticker}: × ×ª×•× ×™× × ×§×™×™×, {len(processed_df)} ×©×•×¨×•×ª")
                    else:
                        # × ×ª×•× ×™× ×’×•×œ××™×™× - ×¦×¨×™×š ×¢×™×‘×•×“ ××œ×
                        processed_df = self._process_raw_to_ohlcv(raw_df, ticker)
                        if processed_df is not None and len(processed_df) > 0:
                            processed_data_map[ticker] = processed_df
                            self.logger.debug(f"ğŸ”„ {ticker}: ×¢×•×‘×“ ×-JSON ×’×•×œ××™, {len(processed_df)} ×©×•×¨×•×ª")
                        else:
                            self.logger.warning(f"âš ï¸ {ticker}: ×›×©×œ×•×Ÿ ×‘×¢×™×‘×•×“ × ×ª×•× ×™× ×’×•×œ××™×™×")
                            
                except Exception as e:
                    self.logger.warning(f"âŒ {ticker}: ×©×’×™××” ×‘×¢×™×‘×•×“ - {e}")
                    continue
            
            # ××’×‘×™×œ ×œ-10 ×˜×™×§×¨×™× ×œ×‘×“×™×§×” ××”×™×¨×”
            limited_data = dict(list(processed_data_map.items())[:10])
            
            self.logger.info(f"âœ… × ×˜×¢× ×• ×•×¢×•×‘×“×• {len(limited_data)} ×˜×™×§×¨×™× ×‘×”×¦×œ×—×”")
            
            # ×‘×“×™×§×” ×©×”× ×ª×•× ×™× ×‘×¤×•×¨××˜ ×”× ×›×•×Ÿ
            for ticker, df in limited_data.items():
                if df is not None and not df.empty:
                    has_ohlcv = all(col in df.columns for col in ['Open', 'High', 'Low', 'Close', 'Volume'])
                    has_date_index = pd.api.types.is_datetime64_any_dtype(df.index)
                    self.logger.debug(f"âœ“ {ticker}: {len(df)} ×©×•×¨×•×ª, OHLCV: {has_ohlcv}, ×ª××¨×™×š: {has_date_index}")
                    break
            
            return limited_data
            
        except Exception as e:
            self.logger.error(f"âŒ ×©×’×™××” ×‘×˜×¢×™× ×ª × ×ª×•× ×™×: {e}")
            import traceback
            self.logger.error(traceback.format_exc())
            return {}
    
    def _is_clean_ohlcv_data(self, df: pd.DataFrame) -> bool:
        """×‘×•×“×§ ×× DataFrame ×›×‘×¨ ××›×™×œ × ×ª×•× ×™× × ×§×™×™× ×‘-OHLCV format"""
        try:
            required_cols = ['Open', 'High', 'Low', 'Close', 'Volume']
            return all(col in df.columns for col in required_cols) and len(df.columns) <= 10
        except:
            return False
    
    def _process_raw_to_ohlcv(self, raw_df: pd.DataFrame, ticker: str) -> Optional[pd.DataFrame]:
        """××¢×‘×“ × ×ª×•× ×™× ×’×•×œ××™×™× (JSON) ×œ×¤×•×¨××˜ OHLCV × ×§×™
        
        ××—×œ×¥ × ×ª×•× ×™ ××—×™×¨ ××”×¢××•×“×” 'price.yahoo.daily' ×•×××™×¨ ×œ×¤×•×¨××˜ ×ª×§× ×™
        """
        try:
            import pandas as pd
            from data.data_utils import _standardize_columns, _ensure_datetime_index, maybe_adjust_with_adj
            
            # ×—×™×¤×•×© ×¢××•×“×ª × ×ª×•× ×™ ××—×™×¨
            price_col = None
            if 'price.yahoo.daily' in raw_df.columns:
                price_col = 'price.yahoo.daily'
            else:
                # ×—×™×¤×•×© ×¢××•×“×•×ª ××—×¨×•×ª ×©×¢×©×•×™×•×ª ×œ×”×›×™×œ × ×ª×•× ×™ ××—×™×¨
                for col in raw_df.columns:
                    if 'price' in str(col).lower() and 'daily' in str(col).lower():
                        price_col = col
                        break
            
            if price_col is None:
                self.logger.debug(f"âš ï¸ {ticker}: ×œ× × ××¦××” ×¢××•×“×ª × ×ª×•× ×™ ××—×™×¨")
                return None
            
            # ×—×™×œ×•×¥ × ×ª×•× ×™ ×”××—×™×¨
            price_data = raw_df[price_col].iloc[0]
            
            # ×”××¨×” ×œ×¨×©×™××” ×× ××’×™×¢ ×›-numpy array
            if hasattr(price_data, 'tolist'):
                price_data = price_data.tolist()
            
            if not isinstance(price_data, (list, tuple)) or len(price_data) == 0:
                self.logger.debug(f"âš ï¸ {ticker}: × ×ª×•× ×™ ××—×™×¨ ×œ× ×‘×¨×©×™××” ××• ×¨×™×§×™× - ×¡×•×’: {type(price_data)}")
                return None
            
            # ×•×™×“×•× ×©×”×¨×©×•××” ×”×¨××©×•× ×” ×”×™× dictionary
            if not isinstance(price_data[0], dict):
                self.logger.debug(f"âš ï¸ {ticker}: ×¤×•×¨××˜ × ×ª×•× ×™ ××—×™×¨ ×œ× ×ª×§×™×Ÿ - ×¨×©×•××” ×¨××©×•× ×”: {type(price_data[0])}")
                return None
            
            # ×™×¦×™×¨×ª DataFrame ×× ×ª×•× ×™ ×”××—×™×¨
            df = pd.DataFrame(price_data)
            self.logger.debug(f"ğŸ”„ {ticker}: ×™×¦×¨ DataFrame ×-{len(price_data)} ×¨×©×•××•×ª ××—×™×¨")
            
            # × ×¨××•×œ ×©××•×ª ×¢××•×“×•×ª (open -> Open, etc.)
            df = _standardize_columns(df)
            
            # ×˜×™×¤×•×œ ×‘××™× ×“×§×¡ ×ª××¨×™×›×™×  
            df = _ensure_datetime_index(df, path=f"ticker_{ticker}")
            
            # ×•×™×“×•× ×©×™×© ×¢××•×“×•×ª OHLCV × ×“×¨×©×•×ª
            required_cols = ['Open', 'High', 'Low', 'Close', 'Volume']
            missing_cols = [col for col in required_cols if col not in df.columns]
            
            if missing_cols:
                self.logger.debug(f"âš ï¸ {ticker}: ×—×¡×¨×•×ª ×¢××•×“×•×ª: {missing_cols}")
                # × ×•×¡×™×£ ×¢××•×“×•×ª ×—×¡×¨×•×ª ×›NaN
                for col in missing_cols:
                    df[col] = pd.NA
            
            # ×”××¨×” ×œ××¡×¤×¨×™×
            for col in required_cols:
                if col in df.columns:
                    df[col] = pd.to_numeric(df[col], errors='coerce')
            
            # ×”×—×œ×¤×ª Close ×‘-Adj Close ×× ×§×™×™×
            df = maybe_adjust_with_adj(df, use_adj=True)
            
            # ×¡×™× ×•×Ÿ ×©×•×¨×•×ª ×¢× × ×ª×•× ×™× ×—×¡×¨×™× ×‘×¢××•×“×•×ª ×§×¨×™×˜×™×•×ª
            before_dropna = len(df)
            df = df.dropna(subset=['Open', 'High', 'Low', 'Close'])
            after_dropna = len(df)
            
            if before_dropna != after_dropna:
                self.logger.debug(f"ğŸ§¹ {ticker}: ×”×¡×™×¨ {before_dropna - after_dropna} ×©×•×¨×•×ª ×¢× × ×ª×•× ×™× ×—×¡×¨×™×")
            
            if len(df) == 0:
                self.logger.warning(f"âš ï¸ {ticker}: ×œ× × ×©××¨×• × ×ª×•× ×™× ×ª×§×™× ×™× ××—×¨×™ × ×™×§×•×™")
                return None
            
            # ×•×•×™×“×•× ×©×™×© ××™× ×“×§×¡ ×ª××¨×™×›×™×
            if not pd.api.types.is_datetime64_any_dtype(df.index):
                self.logger.warning(f"âš ï¸ {ticker}: ××™× ×“×§×¡ ×ª××¨×™×š ×œ× ×ª×§×™×Ÿ")
                return None
            
            # ××™×•×Ÿ ×œ×¤×™ ×ª××¨×™×š
            df = df.sort_index()
            
            self.logger.debug(f"âœ… {ticker}: ×”××¨×” ××•×¦×œ×—×ª - {len(df)} ×©×•×¨×•×ª, {df.index.min()} ×¢×“ {df.index.max()}")
            return df
                
        except Exception as e:
            self.logger.warning(f"âŒ {ticker}: ×©×’×™××” ×‘×¢×™×‘×•×“ × ×ª×•× ×™ ××—×™×¨ - {e}")
            return None
    
    def _train_model_for_date(self, test_date: str, horizon: int, 
                             algorithm: str, all_data: Dict) -> Optional[str]:
        """××××Ÿ ××•×“×œ ×œ×ª××¨×™×š ×¡×¤×¦×™×¤×™"""
        try:
            # ×¡×™× ×•×Ÿ × ×ª×•× ×™× ×¢×“ ×”×ª××¨×™×š ×”× ×“×¨×©
            filtered_data = filter_data_until_date(all_data, test_date)
            
            if len(filtered_data) < 2:  # × ×“×¨×© ××™× ×™××•× × ×ª×•× ×™× (×”×•×§×˜×Ÿ ×-5 ×œ-2)
                self.logger.warning(f"âš ï¸ ××™×Ÿ × ×ª×•× ×™× ××¡×¤×™×§×™× ×œ×ª××¨×™×š {test_date} - ×¨×§ {len(filtered_data)} ×˜×™×§×¨×™×")
                return None
            
            # ××™××•×Ÿ ×”××•×“×œ
            model_filename = f"model_{algorithm}_h{horizon}_{test_date.replace('-', '')}"
            
            # ×©×™××•×© ×‘×¤×•× ×§×¦×™×™×ª ×”××™××•×Ÿ ×”×§×™×™××ª (××—×–×™×¨×” × ×ª×™×‘ ×œ××•×“×œ)
            actual_model_path = train_multi_horizon_model(
                cutoff_date=test_date,
                horizon_days=horizon,
                algorithm=algorithm,
                data_map=filtered_data
            )
            
            if actual_model_path and os.path.exists(actual_model_path):
                self.logger.debug(f"âœ… × ×©××¨ ××•×“×œ: {actual_model_path}")
                return model_filename
            else:
                self.logger.warning(f"âš ï¸ ××™××•×Ÿ × ×›×©×œ ×œ×ª××¨×™×š {test_date}")
                return None
                
        except Exception as e:
            self.logger.error(f"âŒ ×©×’×™××” ×‘××™××•×Ÿ ××•×“×œ: {e}")
            return None
    
    def _run_historical_scan(self, model_path: str, test_date: str, horizon: int) -> Dict:
        """××¨×™×¥ ×¡×¨×™×§×” ×”×™×¡×˜×•×¨×™×ª"""
        try:
            # ×›×¨×’×¢ ××—×–×™×¨ × ×ª×•× ×™× ×“××” - ×‘×¢×ª×™×“ × ×™×ª×Ÿ ×œ×—×‘×¨ ×œ×¡×§×× ×¨ ×”×××™×ª×™
            return {
                'model_path': model_path,
                'test_date': test_date,
                'horizon': horizon,
                'candidates_found': 5,  # ×“××”
                'scan_time': '2024-01-01 10:00:00'  # ×“××”
            }
        except Exception as e:
            self.logger.error(f"âŒ ×©×’×™××” ×‘×¡×¨×™×§×”: {e}")
            return {}
    
    def _save_interim_results(self, results: Dict, current_date: str):
        """×©×•××¨ ×ª×•×¦××•×ª ×‘×™× ×™×™×"""
        try:
            interim_filename = f"interim_results_{current_date.replace('-', '')}.json"
            interim_path = os.path.join(self.results_dir, interim_filename)
            
            with open(interim_path, 'w', encoding='utf-8') as f:
                json.dump(results, f, indent=2, ensure_ascii=False, default=str)
                
        except Exception as e:
            self.logger.warning(f"âš ï¸ ×©×’×™××” ×‘×©××™×¨×ª ×ª×•×¦××•×ª ×‘×™× ×™×™×: {e}")
    
    def _generate_summary(self, results: Dict):
        """×™×•×¦×¨ ×¡×™×›×•× ×”×ª×•×¦××•×ª"""
        daily_results = results['daily_results']
        
        summary = {
            'total_days_tested': len(daily_results),
            'total_models_trained': 0,
            'successful_scans': 0,
            'performance_metrics': {}
        }
        
        for date, date_results in daily_results.items():
            summary['total_models_trained'] += len(date_results)
            summary['successful_scans'] += len([r for r in date_results.values() if r.get('scan_results')])
        
        results['summary'] = summary
    
    def _save_final_results(self, results: Dict) -> str:
        """×©×•××¨ ×ª×•×¦××•×ª ×¡×•×¤×™×•×ª"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = f"backtest_results_{timestamp}.json"
        final_path = os.path.join(self.results_dir, filename)
        
        with open(final_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False, default=str)
        
        return final_path


def main():
    """×“×•×’××” ×œ×©×™××•×© ×‘××¢×¨×›×ª"""
    print("ğŸ§ª ×‘×“×™×§×” ××”×™×¨×” ×©×œ ×”×‘×§×¨ ×”×”×™×¡×˜×•×¨×™...")
    
    backtester = HistoricalBacktester()
    
    # ×‘×“×™×§×” ××”×™×¨×” - 3 ×ª××¨×™×›×™× ×‘×œ×‘×“
    print("ğŸ“… ×‘×•×“×§ ×ª×§×•×¤×”: 2024-01-01 ×¢×“ 2024-01-15")
    results = backtester.run_historical_backtest(
        start_date="2024-01-01",
        end_date="2024-01-15",
        horizons=[1],  # ×¨×§ ×”×•×¨×™×–×•×Ÿ ××—×“
        algorithms=['rf']  # ×¨×§ RF
    )
    
    print("âœ… ×‘×“×™×§×” ×”×•×©×œ××”!")
    print(f"ğŸ“Š ×ª×•×¦××•×ª: {results['summary'].get('total_days_tested', 0)} ×™××™× × ×‘×“×§×•")
    
    print("\nğŸ¯ ×”×‘×§×¨ ×”×”×™×¡×˜×•×¨×™ ××•×›×Ÿ ×œ×©×™××•×©!")


if __name__ == "__main__":
    main()