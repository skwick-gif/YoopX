"""
××¢×¨×›×ª ×‘×“×™×§×” ×”×™×¡×˜×•×¨×™×ª - Phase 2
=====================================

×”×‘×§×¨ ×”×¨××©×™ ×œ×‘×™×¦×•×¢ ×‘×“×™×§×•×ª ×”×™×¡×˜×•×¨×™×•×ª ×¢× ××•×“×œ×™× ××•×ª×××™× ×œ×–××Ÿ.

×ª×›×•× ×•×ª:
- ×‘×“×™×§×” ××•×˜×•××˜×™×ª ×©×œ ×ª×§×•×¤×•×ª ×©×•× ×•×ª
- ××™××•×Ÿ ××•×“×œ×™× ×¢× ×”×•×¨×™×–×•× ×™× ×©×•× ×™× (1/5/10 ×™××™×)
- ×¡×¨×™×§×” ×”×™×¡×˜×•×¨×™×ª ×•×—×™×©×•×‘ ×‘×™×¦×•×¢×™×
- ×”×©×•×•××ª ×ª×•×¦××•×ª ×œ×¤× ×™ ×•××—×¨×™

×”×¢×¨×” ×—×©×•×‘×” - ×–×¨×™××ª ×”× ×ª×•× ×™×:
===============================
1. ×›×¤×ª×•×¨ "Daily Update" â†’ ××•×¨×™×“ × ×ª×•× ×™× ×’×•×œ××™×™× (JSON) ×œ-raw_data
2. Processing Pipeline â†’ ×××™×¨ ×œ-Parquet ××•×‘× ×” ×‘-processed_data/_parquet 
3. ML Training â†’ ××©×ª××© ×‘× ×ª×•× ×™ ×”×¤××¨×§×˜ ×”××¢×•×‘×“×™× (×¤×•×¨××˜: date index + OHLCV columns)
4. Historical Backtesting â†’ ××©×ª××© ×‘××•×ª× × ×ª×•× ×™× ××¢×•×‘×“×™×

× ×ª×™×‘ × ×ª×•× ×™× × ×›×•×Ÿ: get_data_paths()['processed'] + '/_parquet'
×¤×•×¨××˜ × ×ª×•× ×™× × ×“×¨×©: DataFrame ×¢× date index ×•×§×•×œ×•× ×•×ª ['Open','High','Low','Close','Volume']
"""

import os
import sys
import json
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple
import pandas as pd

# ×”×•×¡×¤×ª × ×ª×™×‘ ×”×¤×¨×•×™×§×˜
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from ml.train_model import train_multi_horizon_model, filter_data_until_date
from data.data_utils import load_tickers_on_demand, get_catalog, list_tickers_from_folder
# from backend import scan_symbols_with_ml, calculate_composite_score  # ×œ×‘×“×•×§ ××—×¨ ×›×š


class HistoricalBacktester:
    """×‘×§×¨ ×œ×‘×™×¦×•×¢ ×‘×“×™×§×•×ª ×”×™×¡×˜×•×¨×™×•×ª ×¢× ××•×“×œ×™× ××•×ª×××™×"""
    
    def __init__(self, base_path: Optional[str] = None):
        self.base_path = base_path or os.getcwd()
        self.models_dir = os.path.join(self.base_path, 'ml', 'models', 'historical')
        self.results_dir = os.path.join(self.base_path, 'ml', 'backtest_results')
        
        # ×•×™×“×•× ×©×”×ª×™×§×™×•×ª ×§×™×™××•×ª
        os.makedirs(self.models_dir, exist_ok=True)
        os.makedirs(self.results_dir, exist_ok=True)
        
        # ×”×’×“×¨×ª ×œ×•×’
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)
        
    def run_historical_backtest(self, 
                              start_date: str, 
                              end_date: str, 
                              horizons: List[int] = [1, 5, 10],
                              algorithms: List[str] = ['rf', 'xgb', 'lgbm']) -> Dict:
        """
        ××¨×™×¥ ×‘×“×™×§×” ×”×™×¡×˜×•×¨×™×ª ××œ××”
        
        Args:
            start_date: ×ª××¨×™×š ×”×ª×—×œ×” (YYYY-MM-DD)
            end_date: ×ª××¨×™×š ×¡×™×•× (YYYY-MM-DD) 
            horizons: ×¨×©×™××ª ×”×•×¨×™×–×•× ×™× ×œ×‘×“×™×§×”
            algorithms: ×¨×©×™××ª ××œ×’×•×¨×™×ª××™× ×œ×‘×“×™×§×”
            
        Returns:
            Dict: ×ª×•×¦××•×ª ×”×‘×“×™×§×” ×”×”×™×¡×˜×•×¨×™×ª
        """
        
        self.logger.info(f"ğŸš€ ××ª×—×™×œ ×‘×“×™×§×” ×”×™×¡×˜×•×¨×™×ª: {start_date} ×¢×“ {end_date}")
        
        results = {
            'config': {
                'start_date': start_date,
                'end_date': end_date,
                'horizons': horizons,
                'algorithms': algorithms
            },
            'daily_results': {},
            'summary': {}
        }
        
        # ×˜×¢×™× ×ª × ×ª×•× ×™×
        self.logger.info("ğŸ“¥ ×˜×•×¢×Ÿ × ×ª×•× ×™×...")
        all_data = self._load_all_data()
        
        # ×™×¦×™×¨×ª ×¨×©×™××ª ×ª××¨×™×›×™× ×œ×‘×“×™×§×”
        test_dates = self._generate_test_dates(start_date, end_date)
        
        total_tests = len(test_dates) * len(horizons) * len(algorithms)
        test_count = 0
        
        for test_date in test_dates:
            self.logger.info(f"ğŸ“… ×‘×•×“×§ ×ª××¨×™×š: {test_date}")
            
            date_results = {}
            
            for horizon in horizons:
                for algorithm in algorithms:
                    test_count += 1
                    progress = (test_count / total_tests) * 100
                    
                    self.logger.info(f"ğŸ”„ [{progress:.1f}%] ××™××•×Ÿ {algorithm} ×œhorizon {horizon} ×™××™×")
                    
                    # ××™××•×Ÿ ××•×“×œ ×¢× × ×ª×•× ×™× ×¢×“ ×”×ª××¨×™×š ×”× ×ª×•×Ÿ
                    model_path = self._train_model_for_date(
                        test_date, horizon, algorithm, all_data
                    )
                    
                    if model_path:
                        # ×‘×™×¦×•×¢ ×¡×¨×™×§×” ×¢× ×”××•×“×œ ×”×—×“×©
                        scan_results = self._run_historical_scan(
                            model_path, test_date, horizon
                        )
                        
                        # ×©××™×¨×ª ×ª×•×¦××•×ª
                        key = f"{algorithm}_h{horizon}"
                        date_results[key] = {
                            'model_path': model_path,
                            'scan_results': scan_results
                        }
                    
            results['daily_results'][test_date] = date_results
            
            # ×©××™×¨×” ×‘×™× ×™×™×
            self._save_interim_results(results, test_date)
        
        # ×—×™×©×•×‘ ×¡×™×›×•×
        results['summary'] = self._calculate_summary(results)
        
        # ×©××™×¨×” ×¡×•×¤×™×ª
        final_path = self._save_final_results(results)
        self.logger.info(f"âœ… ×‘×“×™×§×” ×”×•×©×œ××”! ×ª×•×¦××•×ª × ×©××¨×• ×‘: {final_path}")
        
        return results
    
    def _generate_test_dates(self, start_date: str, end_date: str) -> List[str]:
        """×™×•×¦×¨ ×¨×©×™××ª ×ª××¨×™×›×™× ×œ×‘×“×™×§×”"""
        dates = []
        current = pd.Timestamp(start_date)
        end = pd.Timestamp(end_date)
        
        # ×‘×“×™×§×” ×›×œ 5 ×™××™ ×¢×¡×§×™×
        while current <= end:
            if current.weekday() < 5:  # ×™××™ ×¢×¡×§×™× ×‘×œ×‘×“
                dates.append(current.strftime('%Y-%m-%d'))
            
            # ×§×¤×™×¦×” ×©×œ ×©×‘×•×¢
            current += pd.Timedelta(days=7)
            
        return dates
    
    def _load_all_data(self) -> Dict:
        """×˜×•×¢×Ÿ ××ª ×›×œ ×”× ×ª×•× ×™× ××”××¢×¨×›×ª ×”×§×™×™××ª - ×‘×“×™×•×§ ×›××• ×©×”××¢×¨×›×ª ×¢×•×‘×“×ª"""
        try:
            # ×©×™××•×© ×‘× ×ª×™×‘×™ ×”××¢×¨×›×ª ×”×§×™×™××ª
            from data.data_paths import get_data_paths
            from data.enhanced_verification import _load_processed_data_map
            
            paths = get_data_paths()
            processed_dir = paths['processed']
            
            self.logger.info(f"ğŸ“Š ×˜×•×¢×Ÿ × ×ª×•× ×™× ××ª×™×§×™×™×ª ×”××¢×•×‘×“×™×: {processed_dir}")
            
            # ×˜×¢×™× ×” ×‘×“×™×•×§ ×›××• ×©×”××¢×¨×›×ª ×”×§×™×™××ª ×¢×•×©×”
            data_map = _load_processed_data_map(processed_dir)
            
            if not data_map:
                self.logger.warning("âš ï¸ ×œ× × ××¦××• × ×ª×•× ×™× ××¢×•×‘×“×™×. ×”×¨×¥ Daily Update ×ª×—×™×œ×”.")
                return {}
            
            # ××’×‘×™×œ ×œ-10 ×˜×™×§×¨×™× ×œ×‘×“×™×§×”
            limited_data = dict(list(data_map.items())[:10])
            
            self.logger.info(f"âœ… × ×˜×¢× ×• {len(limited_data)} ×˜×™×§×¨×™× ×‘×”×¦×œ×—×”")
            
            # ×‘×“×™×§×” ×©×”× ×ª×•× ×™× ×‘×¤×•×¨××˜ ×”× ×›×•×Ÿ
            for ticker, df in limited_data.items():
                if df is not None and not df.empty:
                    self.logger.debug(f"âœ“ {ticker}: {len(df)} ×©×•×¨×•×ª, ×¢××•×“×•×ª: {list(df.columns)}")
                    break
            
            return limited_data
            
        except Exception as e:
            self.logger.error(f"âŒ ×©×’×™××” ×‘×˜×¢×™× ×ª × ×ª×•× ×™×: {e}")
            return {}

    def _load_parquet_data(self, tickers: List[str], data_dir: str) -> Dict:
        """×˜×•×¢×Ÿ × ×ª×•× ×™× ××§×‘×¦×™ ×¤××¨×§×˜ ×•×”×•×¤×š ××•×ª× ×œ×¤×•×¨××˜ ××—×™×¨×™× ×™×•××™×™×"""
        data = {}
        import pandas as pd
        
        for ticker in tickers:
            try:
                parquet_path = os.path.join(data_dir, f"{ticker}.parquet")
                if os.path.exists(parquet_path):
                    # ×˜×¢×™× ×ª ×”×§×•×‘×¥
                    df = pd.read_parquet(parquet_path)
                    
                    # ×—×™×œ×•×¥ × ×ª×•× ×™ ×”××—×™×¨×™× ××”×©×“×” ×”××ª××™×
                    price_data = df['price.yahoo.daily'].iloc[0]
                    
                    if price_data is not None and len(price_data) > 0:
                        # ×”××¨×” ×œDataFrame
                        price_df = pd.DataFrame(price_data)
                        
                        # ×”××¨×ª ×”×ª××¨×™×š ×œindex
                        price_df['date'] = pd.to_datetime(price_df['date'])
                        price_df.set_index('date', inplace=True)
                        
                        # ×¡×™×“×•×¨ ×”× ×ª×•× ×™×
                        price_df = price_df.sort_index()
                        
                        data[ticker] = price_df
                        self.logger.debug(f"âœ… × ×˜×¢×Ÿ {ticker}: {len(price_df)} ×™××™× ×©×œ × ×ª×•× ×™×")
                    else:
                        self.logger.warning(f"âš ï¸ ××™×Ÿ × ×ª×•× ×™ ××—×™×¨×™× ×œ-{ticker}")
                else:
                    self.logger.warning(f"âš ï¸ ×§×•×‘×¥ ×œ× × ××¦×: {ticker}")
            except Exception as e:
                self.logger.warning(f"âš ï¸ ×©×’×™××” ×‘×˜×¢×™× ×ª {ticker}: {e}")
                
        return data

    def _get_available_tickers(self, data_dir: str) -> List[str]:
        """××—×–×™×¨ ×¨×©×™××ª ×˜×™×§×¨×™× ×–××™× ×™× ××§×‘×¦×™ ×”×¤××¨×§×˜"""
        tickers = []
        
        if not os.path.exists(data_dir):
            self.logger.error(f"âŒ ×ª×™×§×™×™×ª × ×ª×•× ×™× ×œ× ×§×™×™××ª: {data_dir}")
            return tickers
            
        for filename in os.listdir(data_dir):
            if filename.endswith('.parquet'):
                ticker = filename.replace('.parquet', '')
                tickers.append(ticker)
                
        self.logger.info(f"ğŸ¯ × ××¦××• {len(tickers)} ×˜×™×§×¨×™× ×–××™× ×™×")
        return sorted(tickers)
    
    def _train_model_for_date(self, test_date: str, horizon: int, 
                             algorithm: str, all_data: Dict) -> Optional[str]:
        """××××Ÿ ××•×“×œ ×¢× × ×ª×•× ×™× ×¢×“ ×ª××¨×™×š ××¡×•×™×"""
        
        try:
            # ×¡×™× ×•×Ÿ × ×ª×•× ×™× ×¢×“ ×”×ª××¨×™×š
            filtered_data = filter_data_until_date(all_data, test_date)
            
            if not filtered_data:
                self.logger.warning(f"âš ï¸ ××™×Ÿ × ×ª×•× ×™× ××¡×¤×™×§×™× ×œ×ª××¨×™×š {test_date}")
                return None
            
            # ××™××•×Ÿ ×”××•×“×œ
            model_path = train_multi_horizon_model(
                filtered_data, horizon, algorithm, 
                save_path=os.path.join(
                    self.models_dir, 
                    f"{test_date}_{algorithm}_h{horizon}.joblib"
                )
            )
            
            return model_path
            
        except Exception as e:
            self.logger.error(f"âŒ ×©×’×™××” ×‘××™××•×Ÿ ××•×“×œ: {e}")
            return None
    
    def _run_historical_scan(self, model_path: str, test_date: str, 
                           horizon: int) -> Dict:
        """××¨×™×¥ ×¡×¨×™×§×” ×”×™×¡×˜×•×¨×™×ª ×¢× ××•×“×œ × ×ª×•×Ÿ"""
        
        try:
            # ×›××Ÿ × ×¦×˜×¨×š ×œ×©× ×•×ª ××ª backend.py ×œ×§×‘×œ × ×ª×™×‘ ××•×“×œ
            # ×‘×™× ×ª×™×™× × ×—×–×™×¨ ××‘× ×” ×‘×¡×™×¡×™
            
            # TODO: ×©×™× ×•×™ backend.py ×œ×ª××•×š ×‘××•×“×œ×™× ×”×™×¡×˜×•×¨×™×™×
            
            return {
                'date': test_date,
                'horizon': horizon,
                'model_path': model_path,
                'top_symbols': [],  # ×™×™××œ× ××—×¨×™ ×©× ×©× ×” ××ª backend
                'scan_count': 0,
                'success': True
            }
            
        except Exception as e:
            self.logger.error(f"âŒ ×©×’×™××” ×‘×¡×¨×™×§×”: {e}")
            return {
                'date': test_date,
                'horizon': horizon,
                'model_path': model_path,
                'success': False,
                'error': str(e)
            }
    
    def _save_interim_results(self, results: Dict, date: str):
        """×©×•××¨ ×ª×•×¦××•×ª ×‘×™× ×™×™×"""
        interim_path = os.path.join(
            self.results_dir, 
            f"interim_results_{date}.json"
        )
        
        try:
            with open(interim_path, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
        except Exception as e:
            self.logger.warning(f"âš ï¸ ×œ× ×”×¦×œ×—×ª×™ ×œ×©××•×¨ ×ª×•×¦××•×ª ×‘×™× ×™×™×: {e}")
    
    def _calculate_summary(self, results: Dict) -> Dict:
        """××—×©×‘ ×¡×™×›×•× ×”×ª×•×¦××•×ª"""
        summary = {
            'total_tests': len(results['daily_results']),
            'successful_tests': 0,
            'failed_tests': 0,
            'performance_by_horizon': {},
            'performance_by_algorithm': {}
        }
        
        # ×¡×¤×™×¨×ª ×‘×“×™×§×•×ª ××•×¦×œ×—×•×ª
        for date_results in results['daily_results'].values():
            for test_result in date_results.values():
                if test_result.get('scan_results', {}).get('success', False):
                    summary['successful_tests'] += 1
                else:
                    summary['failed_tests'] += 1
        
        # TODO: ×—×™×©×•×‘×™ ×‘×™×¦×•×¢×™× ××¤×•×¨×˜×™× ×œ××—×¨ ×™×™×©×•× ×”×¡×¨×™×§×” ×”××œ××”
        
        return summary
    
    def _save_final_results(self, results: Dict) -> str:
        """×©×•××¨ ×ª×•×¦××•×ª ×¡×•×¤×™×•×ª"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        final_path = os.path.join(
            self.results_dir,
            f"backtest_results_{timestamp}.json"
        )
        
        with open(final_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
            
        return final_path


def run_quick_test():
    """×‘×“×™×§×” ××”×™×¨×” ×©×œ ×”××¢×¨×›×ª"""
    print("ğŸ§ª ×‘×“×™×§×” ××”×™×¨×” ×©×œ ×”×‘×§×¨ ×”×”×™×¡×˜×•×¨×™...")
    
    backtester = HistoricalBacktester()
    
    # ×‘×“×™×§×” ×¢× ×ª×§×•×¤×” ×§×¦×¨×”
    start_date = "2024-01-01"
    end_date = "2024-01-15"
    
    print(f"ğŸ“… ×‘×•×“×§ ×ª×§×•×¤×”: {start_date} ×¢×“ {end_date}")
    
    # ×‘×“×™×§×” ×¢× ××•×“×œ ××—×“ ×‘×œ×‘×“
    results = backtester.run_historical_backtest(
        start_date=start_date,
        end_date=end_date,
        horizons=[1],  # ×¨×§ horizon ××—×“
        algorithms=['rf']  # ×¨×§ RF
    )
    
    print("âœ… ×‘×“×™×§×” ×”×•×©×œ××”!")
    print(f"ğŸ“Š ×ª×•×¦××•×ª: {len(results['daily_results'])} ×™××™× × ×‘×“×§×•")
    
    return results


if __name__ == "__main__":
    # ×”×¨×¦×ª ×‘×“×™×§×” ××”×™×¨×”
    results = run_quick_test()
    print("\nğŸ¯ ×”×‘×§×¨ ×”×”×™×¡×˜×•×¨×™ ××•×›×Ÿ ×œ×©×™××•×©!")