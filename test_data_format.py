#!/usr/bin/env python3
"""
×‘×“×™×§×ª ×¤×•×¨×ž×˜ ×”× ×ª×•× ×™× ×”×ž×¢×•×‘×“×™× - ×›×“×™ ×œ×”×‘×™×Ÿ ××™×š ×”×ž×¢×¨×›×ª ×”×§×™×™×ž×ª ×¢×•×‘×“×ª

×¡×§×¨×™×¤×˜ ×–×” ×‘×•×“×§:
1. ××™×š × ×¨××™× ×§×‘×¦×™ PARQUET ×ž×¢×•×‘×“×™×
2. ××™×š ×”×ž×¢×¨×›×ª ×”×§×™×™×ž×ª ×˜×•×¢× ×ª ××•×ª×
3. ×ž×” ×”×¤×•×¨×ž×˜ ×”×¡×•×¤×™ ×©×ž×’×™×¢ ×œ-ML
"""

import os
import pandas as pd
from data.data_paths import get_data_paths
from data.enhanced_verification import _load_processed_data_map

def main():
    """×‘×“×™×§×ª ×”×¤×™×¤×œ×™×™×Ÿ ×”×§×™×™×"""
    
    print("ðŸ” ×‘×•×“×§ ×¤×•×¨×ž×˜ × ×ª×•× ×™× ×‘×ž×¢×¨×›×ª ×”×§×™×™×ž×ª...")
    
    # 1. ×§×‘×œ×ª × ×ª×™×‘×™ ×”×ž×¢×¨×›×ª
    paths = get_data_paths()
    processed_dir = paths['processed']
    
    print(f"ðŸ“ ×ª×™×§×™×™×ª × ×ª×•× ×™× ×ž×¢×•×‘×“×™×: {processed_dir}")
    
    # 2. ×‘×“×™×§×ª ×ž×‘× ×” ×”×ª×™×§×™×”  
    parquet_dir = os.path.join(processed_dir, '_parquet')
    catalog_dir = os.path.join(processed_dir, '_catalog')
    
    print(f"ðŸ“‹ ×ª×™×§×™×™×ª PARQUET: {parquet_dir} (×§×™×™×: {os.path.exists(parquet_dir)})")
    print(f"ðŸ“‹ ×ª×™×§×™×™×ª CATALOG: {catalog_dir} (×§×™×™×: {os.path.exists(catalog_dir)})")
    
    if os.path.exists(parquet_dir):
        files = [f for f in os.listdir(parquet_dir) if f.endswith('.parquet')]
        print(f"ðŸ“Š ×ž×¡×¤×¨ ×§×‘×¦×™ PARQUET: {len(files)}")
        
        if files:
            # ×‘×“×™×§×ª ×§×•×‘×¥ ×“×•×’×ž×” ×‘×¤×•×¨×ž×˜ ×’×•×œ×ž×™
            sample_file = files[0]
            sample_path = os.path.join(parquet_dir, sample_file)
            ticker = sample_file[:-8]
            
            print(f"\nðŸ” ×‘×•×“×§ ×§×•×‘×¥ ×“×•×’×ž×”: {ticker}")
            
            # ×˜×¢×™× ×” ×™×©×™×¨×” ×©×œ PARQUET (×œ×œ× ×¢×™×‘×•×“)
            df_raw = pd.read_parquet(sample_path)
            print(f"ðŸ“Š ×¤×•×¨×ž×˜ ×’×•×œ×ž×™ - ×©×•×¨×•×ª: {len(df_raw)}, ×¢×ž×•×“×•×ª: {list(df_raw.columns)}")
            print(f"ðŸ§© ×“×•×’×ž×ª × ×ª×•× ×™× ×’×•×œ×ž×™×™×:")
            print(df_raw.head(2).to_string())
            
            # ×‘×“×™×§×” ×× ×–×” JSON ×’×•×œ×ž×™
            if len(df_raw.columns) == 1:
                col_name = df_raw.columns[0]
                sample_value = df_raw.iloc[0, 0]
                print(f"\nðŸ” ×¢×ž×•×“×” ×™×—×™×“×” '{col_name}' ×ž×›×™×œ×”: {type(sample_value)}")
                if isinstance(sample_value, (dict, str)):
                    print(f"ðŸ“ ×ª×•×›×Ÿ: {sample_value}")
    
    # 3. ×‘×“×™×§×ª ×˜×¢×™× ×” ×“×¨×š ×”×ž×¢×¨×›×ª ×”×§×™×™×ž×ª
    print(f"\nðŸ”„ ×˜×•×¢×Ÿ ×“×¨×š ×”×ž×¢×¨×›×ª ×”×§×™×™×ž×ª (_load_processed_data_map)...")
    
    try:
        data_map = _load_processed_data_map(processed_dir)
        
        if data_map:
            print(f"âœ… × ×˜×¢× ×• {len(data_map)} ×˜×™×§×¨×™×")
            
            # ×‘×“×™×§×ª ×“×•×’×ž×” ×ž×¢×•×‘×“×”
            sample_ticker = list(data_map.keys())[0]
            sample_df = data_map[sample_ticker]
            
            print(f"\nðŸ“Š ×“×•×’×ž×” ×ž×¢×•×‘×“×ª ({sample_ticker}):")
            print(f"   ×©×•×¨×•×ª: {len(sample_df)}")
            print(f"   ×¢×ž×•×“×•×ª: {list(sample_df.columns)}")
            print(f"   ××™× ×“×§×¡: {type(sample_df.index)} ({sample_df.index.name})")
            
            # ×‘×“×™×§×ª ×¢×ž×•×“×•×ª OHLCV
            required_cols = ['Open', 'High', 'Low', 'Close', 'Volume']
            has_ohlcv = all(col in sample_df.columns for col in required_cols)
            print(f"   ×¢×ž×•×“×•×ª OHLCV: {'âœ…' if has_ohlcv else 'âŒ'}")
            
            if has_ohlcv:
                print(f"   ×˜×•×•×— ×ª××¨×™×›×™×: {sample_df.index.min()} ×¢×“ {sample_df.index.max()}")
                print(f"   ×“×•×’×ž×ª × ×ª×•× ×™×:")
                print(sample_df[required_cols].head(3).to_string())
                
                # ×‘×“×™×§×” ×× ×™×© ××™× ×“×§×¡ ×ª××¨×™×š
                if pd.api.types.is_datetime64_any_dtype(sample_df.index):
                    print(f"   âœ… ××™× ×“×§×¡ ×ª××¨×™×š ×ª×§×™×Ÿ")
                else:
                    print(f"   âš ï¸ ××™× ×“×§×¡ ×ª××¨×™×š ×œ× ×ª×§×™×Ÿ: {type(sample_df.index)}")
            else:
                print(f"   âš ï¸ ×—×¡×¨×•×ª ×¢×ž×•×“×•×ª OHLCV, ×ž×¦×™×’ ×“×•×’×ž×”:")
                print(sample_df.head(2).to_string())
                
        else:
            print("âŒ ×œ× × ×˜×¢× ×• × ×ª×•× ×™×")
            
    except Exception as e:
        print(f"âŒ ×©×’×™××” ×‘×˜×¢×™× ×”: {e}")
        import traceback
        traceback.print_exc()
    
    # 4. ×”×¡×‘×¨ ×¢×œ ×”×¤×™×¤×œ×™×™×Ÿ
    print(f"\nðŸ“š ×¡×™×›×•× ×”×¤×™×¤×œ×™×™×Ÿ ×”×§×™×™×:")
    print(f"   1. Daily Update button -> downloads raw JSON")
    print(f"   2. processing_pipeline.py -> converts JSON to PARQUET") 
    print(f"   3. enhanced_verification.py -> loads PARQUET with data transformations")
    print(f"   4. main_content.py -> uses load_parquet_folder + maybe_adjust_with_adj")
    print(f"   5. ML training -> gets clean OHLCV DataFrame with date index")


if __name__ == "__main__":
    main()