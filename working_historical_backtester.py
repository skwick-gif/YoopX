#!/usr/bin/env python3
"""
ğŸ¯ ×”×‘×§×˜×¡×˜×¨ ×”×”×™×¡×˜×•×¨×™ ×”× ×›×•×Ÿ - ×‘×“×™×•×§ ×›××• ×”××¢×¨×›×ª ×”×§×™×™××ª!
==============================================================

×”×©×œ×‘ ×”×‘×: ×™×¦×™×¨×ª ×‘×§×˜×¡×˜×¨ ×©×¢×•×‘×“ ×‘×“×™×•×§ ×›××• ×”×¡×¨×™×§×” ×”×¨×’×™×œ×”,
××‘×œ ×¢×œ × ×ª×•× ×™× ×”×™×¡×˜×•×¨×™×™×.
"""

import os
import json
import pandas as pd
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any
import logging

# ×™×‘×•× ×”×¤×•× ×§×¦×™×•×ª ×”××¨×›×–×™×•×ª ××”××¢×¨×›×ª ×”×§×™×™××ª
from ml.train_model import train_model, filter_data_until_date
from data.enhanced_verification import _load_processed_data_map
from data.data_paths import get_data_paths
from data.data_utils import maybe_adjust_with_adj

class WorkingHistoricalBacktester:
    """
    ×‘×§×˜×¡×˜×¨ ×”×™×¡×˜×•×¨×™ ×©×¢×•×‘×“ ×‘×“×™×•×§ ×›××• ×”××¢×¨×›×ª ×”×§×™×™××ª
    """
    
    def __init__(self):
        """××ª×—×•×œ ×”×‘×§×˜×¡×˜×¨"""
        
        self.logger = logging.getLogger(__name__)
        logging.basicConfig(level=logging.INFO)
        
        # ×ª×™×§×™×•×ª ×ª×•×¦××•×ª
        self.results_dir = "ml/backtest_results"
        os.makedirs(self.results_dir, exist_ok=True)
        
        self.temp_models_dir = "ml/temp_historical_models"
        os.makedirs(self.temp_models_dir, exist_ok=True)
        
    def run_simple_backtest(self, test_date: str = "2025-06-01") -> Dict:
        """
        ×”×¨×¦×” ×¤×©×•×˜×” ×©×œ ×‘×“×™×§×” ×”×™×¡×˜×•×¨×™×ª ×™×—×™×“×”
        """
        
        self.logger.info(f"ğŸš€ ×‘×“×™×§×” ×”×™×¡×˜×•×¨×™×ª ×¤×©×•×˜×” ×œ×ª××¨×™×š: {test_date}")
        
        results = {
            'config': {
                'test_date': test_date,
                'timestamp': datetime.now().isoformat()
            },
            'steps': {},
            'success': False
        }
        
        try:
            # ×©×œ×‘ 1: ×˜×¢×™× ×ª × ×ª×•× ×™× - ×‘×“×™×•×§ ×›××• ×”××¢×¨×›×ª ×”×§×™×™××ª
            self.logger.info("ğŸ“Š ×©×œ×‘ 1: ×˜×•×¢×Ÿ data_map...")
            data_map = self._load_data_map_like_main_system()
            
            if not data_map:
                results['error'] = "×œ× × ×˜×¢× ×• × ×ª×•× ×™×"
                return results
                
            results['steps']['data_loaded'] = {
                'tickers_count': len(data_map),
                'tickers': list(data_map.keys())
            }
            self.logger.info(f"âœ… × ×˜×¢× ×• {len(data_map)} ×˜×™×§×¨×™×: {list(data_map.keys())}")
            
            # ×©×œ×‘ 2: ×¡×™× ×•×Ÿ × ×ª×•× ×™× ×¢×“ ×”×ª××¨×™×š - ×‘×“×™×•×§ ×›××• filter_data_until_date
            self.logger.info(f"ğŸ”„ ×©×œ×‘ 2: ×¡×™× ×•×Ÿ × ×ª×•× ×™× ×¢×“ {test_date}...")
            filtered_data = filter_data_until_date(data_map, test_date)
            
            if not filtered_data:
                results['error'] = "×œ× × ×©××¨×• × ×ª×•× ×™× ××—×¨×™ ×¡×™× ×•×Ÿ"
                return results
                
            results['steps']['data_filtered'] = {
                'tickers_count': len(filtered_data),
                'sample_ranges': {}
            }
            
            # ×‘×“×™×§×” ×©×œ ×˜×•×•×—×™ × ×ª×•× ×™×
            for ticker, df in filtered_data.items():
                if df is not None and not df.empty:
                    results['steps']['data_filtered']['sample_ranges'][ticker] = {
                        'rows': len(df),
                        'date_range': f"{df.index.min()} - {df.index.max()}"
                    }
            
            self.logger.info(f"âœ… ××—×¨×™ ×¡×™× ×•×Ÿ: {len(filtered_data)} ×˜×™×§×¨×™×")
            
            # ×©×œ×‘ 3: ××™××•×Ÿ ××•×“×œ - ×‘×“×™×•×§ ×›××• train_model ×‘××¢×¨×›×ª ×”×§×™×™××ª
            self.logger.info("ğŸ§  ×©×œ×‘ 3: ××™××•×Ÿ ××•×“×œ...")
            
            temp_model_path = os.path.join(self.temp_models_dir, f"temp_model_{test_date.replace('-', '')}.pkl")
            
            training_result = train_model(
                data_map=filtered_data,
                model='rf',  # ×”×ª×—×œ ×¢× RF
                model_path=temp_model_path
            )
            
            if training_result.get('error'):
                results['error'] = f"×©×’×™××” ×‘××™××•×Ÿ: {training_result['error']}"
                return results
            
            results['steps']['model_trained'] = {
                'model_path': temp_model_path,
                'model_exists': os.path.exists(temp_model_path),
                'training_meta': {
                    'validation_auc': training_result.get('validation', {}).get('auc'),
                    'dataset_size': training_result.get('dataset_size'),
                    'features_count': len(training_result.get('features', []))
                }
            }
            
            self.logger.info(f"âœ… ××•×“×œ ××•××Ÿ: AUC = {training_result.get('validation', {}).get('auc', 'N/A')}")
            
            # ×©×œ×‘ 4: ×‘×“×™×§×” ×©×”××•×“×œ ×¢×•×‘×“
            if os.path.exists(temp_model_path):
                from ml.train_model import load_model
                
                model = load_model(temp_model_path)
                if model:
                    self.logger.info("âœ… ××•×“×œ × ×˜×¢×Ÿ ×‘×”×¦×œ×—×” ×•××•×›×Ÿ ×œ×©×™××•×©")
                    results['steps']['model_verified'] = True
                else:
                    results['error'] = "××•×“×œ ×œ× × ×˜×¢×Ÿ ×›×¨××•×™"
                    return results
            
            # ×”×¦×œ×—×”!
            results['success'] = True
            self.logger.info("ğŸ‰ ×‘×“×™×§×” ×”×™×¡×˜×•×¨×™×ª ×”×•×©×œ××” ×‘×”×¦×œ×—×”!")
            
        except Exception as e:
            results['error'] = str(e)
            self.logger.error(f"âŒ ×©×’×™××”: {e}")
            import traceback
            results['traceback'] = traceback.format_exc()
        
        finally:
            # × ×™×§×•×™ ××•×“×œ×™× ×–×× ×™×™×
            try:
                if 'temp_model_path' in locals() and os.path.exists(temp_model_path):
                    os.remove(temp_model_path)
            except:
                pass
        
        return results
    
    def _load_data_map_like_main_system(self) -> Dict[str, pd.DataFrame]:
        """
        ×˜×•×¢×Ÿ data_map ×‘×“×™×•×§ ×›××• ×©×”××¢×¨×›×ª ×”×§×™×™××ª ×¢×•×©×” ×–××ª
        """
        try:
            # ×”×©×ª××© ×‘×“×™×•×§ ×‘××•×ª×Ÿ ×¤×•× ×§×¦×™×•×ª ×©×”××¢×¨×›×ª ×”×§×™×™××ª ××©×ª××©×ª
            paths = get_data_paths()
            processed_dir = paths['processed']
            
            self.logger.info(f"ğŸ“ ×˜×•×¢×Ÿ ××ª×™×§×™×™×ª ×”××¢×•×‘×“×™×: {processed_dir}")
            
            # ×˜×¢×™× ×” ×›××• ×‘××¢×¨×›×ª ×”×§×™×™××ª
            raw_data_map = _load_processed_data_map(processed_dir)
            
            if not raw_data_map:
                self.logger.warning("âš ï¸ ×œ× × ××¦××• × ×ª×•× ×™× ××¢×•×‘×“×™×")
                return {}
            
            # ×¢×™×‘×•×“ ×›××• ×‘××¢×¨×›×ª ×”×§×™×™××ª
            processed_data_map = {}
            
            for ticker, df in raw_data_map.items():
                try:
                    # ×‘×“×™×§×” ×× ×”× ×ª×•× ×™× ×›×‘×¨ ××¢×•×‘×“×™× (×™×© ×¢××•×“×•×ª OHLCV)
                    required_cols = ['Open', 'High', 'Low', 'Close', 'Volume']
                    has_ohlcv = all(col in df.columns for col in required_cols)
                    
                    if has_ohlcv:
                        # × ×ª×•× ×™× ×›×‘×¨ × ×§×™×™× - ×¨×§ maybe_adjust_with_adj
                        final_df = maybe_adjust_with_adj(df.copy(), use_adj=True)
                        processed_data_map[ticker] = final_df
                        self.logger.debug(f"âœ“ {ticker}: × ×ª×•× ×™× × ×§×™×™×, {len(final_df)} ×©×•×¨×•×ª")
                    else:
                        # × ×ª×•× ×™× ×’×•×œ××™×™× - ×¢×™×‘×•×“ ×›××• ×‘×‘×§×˜×¡×˜×¨ ×”×§×•×“×
                        processed_df = self._process_raw_parquet_to_ohlcv(df, ticker)
                        if processed_df is not None and len(processed_df) > 0:
                            processed_data_map[ticker] = processed_df
                            self.logger.debug(f"ğŸ”„ {ticker}: ×¢×™×‘×“ ××’×•×œ××™, {len(processed_df)} ×©×•×¨×•×ª")
                        else:
                            self.logger.debug(f"âš ï¸ {ticker}: ×›×©×œ×•×Ÿ ×‘×¢×™×‘×•×“ × ×ª×•× ×™× ×’×•×œ××™×™×")
                        
                except Exception as e:
                    self.logger.warning(f"âŒ {ticker}: ×©×’×™××” ×‘×¢×™×‘×•×“ - {e}")
                    continue
            
            self.logger.info(f"âœ… ×¢×™×‘×“ {len(processed_data_map)} ×˜×™×§×¨×™× ×‘×”×¦×œ×—×”")
            return processed_data_map
            
        except Exception as e:
            self.logger.error(f"âŒ ×©×’×™××” ×‘×˜×¢×™× ×ª data_map: {e}")
            return {}
    
    def _process_raw_parquet_to_ohlcv(self, raw_df: pd.DataFrame, ticker: str) -> Optional[pd.DataFrame]:
        """
        ××¢×‘×“ × ×ª×•× ×™× ×’×•×œ××™×™× ×-PARQUET ×œ×¤×•×¨××˜ OHLCV
        ××‘×•×¡×¡ ×¢×œ ×”×§×•×“ ××”×‘×§×˜×¡×˜×¨ ×”×§×•×“×
        """
        try:
            from data.data_utils import _standardize_columns, _ensure_datetime_index
            
            # ×—×™×¤×•×© ×¢××•×“×ª × ×ª×•× ×™ ××—×™×¨
            price_col = None
            if 'price.yahoo.daily' in raw_df.columns:
                price_col = 'price.yahoo.daily'
            else:
                # ×—×™×¤×•×© ×¢××•×“×•×ª ××—×¨×•×ª
                for col in raw_df.columns:
                    if 'price' in str(col).lower() and 'daily' in str(col).lower():
                        price_col = col
                        break
            
            if price_col is None:
                self.logger.debug(f"âš ï¸ {ticker}: ×œ× × ××¦××” ×¢××•×“×ª × ×ª×•× ×™ ××—×™×¨")
                return None
            
            # ×—×™×œ×•×¥ × ×ª×•× ×™ ×”××—×™×¨
            price_data = raw_df[price_col].iloc[0]
            
            # ×”××¨×” ×œ×¨×©×™××” ×× ××’×™×¢ ×›-numpy array
            if hasattr(price_data, 'tolist'):
                price_data = price_data.tolist()
            
            if not isinstance(price_data, (list, tuple)) or len(price_data) == 0:
                self.logger.debug(f"âš ï¸ {ticker}: × ×ª×•× ×™ ××—×™×¨ ×œ× ×‘×¨×©×™××” ××• ×¨×™×§×™×")
                return None
            
            # ×•×™×“×•× ×©×”×¨×©×•××” ×”×¨××©×•× ×” ×”×™× dictionary
            if not isinstance(price_data[0], dict):
                self.logger.debug(f"âš ï¸ {ticker}: ×¤×•×¨××˜ × ×ª×•× ×™ ××—×™×¨ ×œ× ×ª×§×™×Ÿ")
                return None
            
            # ×™×¦×™×¨×ª DataFrame ×× ×ª×•× ×™ ×”××—×™×¨
            df = pd.DataFrame(price_data)
            
            # × ×¨××•×œ ×©××•×ª ×¢××•×“×•×ª
            df = _standardize_columns(df)
            
            # ×˜×™×¤×•×œ ×‘××™× ×“×§×¡ ×ª××¨×™×›×™×
            df = _ensure_datetime_index(df, path=f"ticker_{ticker}")
            
            # ×•×™×“×•× ×©×™×© ×¢××•×“×•×ª OHLCV × ×“×¨×©×•×ª
            required_cols = ['Open', 'High', 'Low', 'Close', 'Volume']
            missing_cols = [col for col in required_cols if col not in df.columns]
            
            if missing_cols:
                self.logger.debug(f"âš ï¸ {ticker}: ×—×¡×¨×•×ª ×¢××•×“×•×ª: {missing_cols}")
                # × ×•×¡×™×£ ×¢××•×“×•×ª ×—×¡×¨×•×ª ×›NaN
                for col in missing_cols:
                    df[col] = pd.NA
            
            # ×”××¨×” ×œ××¡×¤×¨×™×
            for col in required_cols:
                if col in df.columns:
                    df[col] = pd.to_numeric(df[col], errors='coerce')
            
            # ×•×•×™×“×•× ×©×”××™× ×“×§×¡ ×ª××¨×™×›×™× ×ª×§×™×Ÿ
            if not pd.api.types.is_datetime64_any_dtype(df.index):
                self.logger.warning(f"âš ï¸ {ticker}: ××™× ×“×§×¡ ×ª××¨×™×š ×œ× ×ª×§×™×Ÿ")
                return None
            
            # ××™×•×Ÿ ×œ×¤×™ ×ª××¨×™×š
            df = df.sort_index()
            
            # ×”×—×œ×ª adjustment
            df = maybe_adjust_with_adj(df, use_adj=True)
            
            self.logger.debug(f"âœ… {ticker}: ×”××¨×” ××•×¦×œ×—×ª - {len(df)} ×©×•×¨×•×ª, {df.index.min()} ×¢×“ {df.index.max()}")
            return df
                
        except Exception as e:
            self.logger.warning(f"âŒ {ticker}: ×©×’×™××” ×‘×¢×™×‘×•×“ × ×ª×•× ×™ ××—×™×¨ - {e}")
            return None

def main():
    """×¤×•× ×§×¦×™×” ×¨××©×™×ª"""
    
    print("ğŸ¯ ×‘×§×˜×¡×˜×¨ ×”×™×¡×˜×•×¨×™ ×—×“×© - ×›××• ×”××¢×¨×›×ª ×”×§×™×™××ª")
    print("=" * 60)
    
    backtester = WorkingHistoricalBacktester()
    
    # ×‘×“×™×§×” ×¤×©×•×˜×”
    results = backtester.run_simple_backtest("2025-06-01")
    
    print(f"\nğŸ“Š ×ª×•×¦××•×ª:")
    print(f"   âœ… ×”×¦×œ×—×”: {results['success']}")
    
    if results['success']:
        steps = results.get('steps', {})
        
        if 'data_loaded' in steps:
            data_info = steps['data_loaded']
            print(f"   ğŸ“‚ × ×ª×•× ×™× × ×˜×¢× ×•: {data_info['tickers_count']} ×˜×™×§×¨×™×")
        
        if 'model_trained' in steps:
            model_info = steps['model_trained']
            training_meta = model_info.get('training_meta', {})
            print(f"   ğŸ§  ××•×“×œ ××•××Ÿ: AUC = {training_meta.get('validation_auc', 'N/A')}")
            print(f"   ğŸ“Š ×’×•×“×œ dataset: {training_meta.get('dataset_size', 'N/A')}")
            
        print(f"\nğŸ‰ ×”×‘×§×˜×¡×˜×¨ ×¢×•×‘×“! ×”××¢×¨×›×ª ××•×›× ×” ×œ×‘×“×™×§×•×ª ××•×¨×—×‘×•×ª")
        
    else:
        print(f"   âŒ ×©×’×™××”: {results.get('error', '×œ× ×™×“×•×¢×”')}")
        
    print(f"\nğŸ”„ ×©×œ×‘×™× ×”×‘××™×:")
    print("1. ğŸ“… ×”×¨×¦×” ×¢×œ ×ª××¨×™×›×™× ×©×•× ×™×")  
    print("2. ğŸ”„ ×”×©×•×•××ª ××•×“×œ×™× (RF vs XGB vs LGBM)")
    print("3. ğŸ“Š × ×™×ª×•×— ×‘×™×¦×•×¢×™×")
    print("4. ğŸ¯ ×”×¨×¦×ª ×¡×¨×™×§×•×ª ×¢×œ ×”× ×ª×•× ×™× ×”×”×™×¡×˜×•×¨×™×™×")

if __name__ == "__main__":
    main()